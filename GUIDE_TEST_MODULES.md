# üß™ Guide de Test - Modules MLOps

## üìã Vue d'ensemble

Ce guide vous permet de tester chaque module de votre projet pour v√©rifier que tout fonctionne avant la soutenance.

---

## ‚úÖ MODULE 1 : Entra√Ænement du Mod√®le

### Test 1 : V√©rifier les d√©pendances

```bash
# Activer l'environnement virtuel
# Windows :
venv\Scripts\activate
# Mac/Linux :
source venv/bin/activate

# V√©rifier que Python fonctionne
python --version
# Doit afficher Python 3.9.x ou sup√©rieur

# V√©rifier les packages install√©s
pip list | grep -E "scikit-learn|mlflow|pandas|numpy"
```

### Test 2 : V√©rifier le dataset

```bash
# V√©rifier que le dataset existe
python -c "import pandas as pd; df = pd.read_csv('data/bank_churn.csv'); print(f'Dataset: {len(df)} lignes, {len(df.columns)} colonnes')"
```

**R√©sultat attendu :**
```
Dataset: 10000 lignes, 11 colonnes
```

### Test 3 : Entra√Æner le mod√®le

```bash
# Ex√©cuter le script d'entra√Ænement
python train_model.py
```

**R√©sultat attendu :**
```
Chargement des donnees...
Dataset : 10000 lignes, 11 colonnes
Taux de churn : XX.XX%

Train : 8000 lignes
Test : 2000 lignes

Entrainement du modele...

==================================================
RESULTATS DE L'ENTRAINEMENT
==================================================
Accuracy  : 0.XXXX
Precision : 0.XXXX
Recall    : 0.XXXX
F1 Score  : 0.XXXX
ROC AUC   : 0.XXXX
==================================================

Modele sauvegarde dans : model/churn_model.pkl
MLflow UI : mlflow ui --port 5000
```

### Test 4 : V√©rifier MLflow

```bash
# V√©rifier que MLflow a cr√©√© les runs
ls mlruns/

# D√©marrer MLflow UI
mlflow ui --port 5000
```

**Actions √† faire :**
1. Ouvrir http://localhost:5000 dans le navigateur
2. V√©rifier qu'il y a des runs d'entra√Ænement
3. Cliquer sur un run pour voir les m√©triques
4. V√©rifier les artefacts (confusion_matrix.png, feature_importance.png)

### Test 5 : V√©rifier le mod√®le sauvegard√©

```bash
# V√©rifier que le mod√®le existe
python -c "import joblib; model = joblib.load('model/churn_model.pkl'); print('Mod√®le charg√© avec succ√®s'); print(f'Type: {type(model)}')"
```

**R√©sultat attendu :**
```
Mod√®le charg√© avec succ√®s
Type: <class 'sklearn.ensemble._forest.RandomForestClassifier'>
```

---

## ‚úÖ MODULE 2 : API FastAPI

### Test 1 : V√©rifier les d√©pendances API

```bash
# V√©rifier FastAPI et uvicorn
pip list | grep -E "fastapi|uvicorn|pydantic"
```

### Test 2 : D√©marrer l'API localement

```bash
# D√©marrer l'API
uvicorn app.main:app --reload --port 8000
```

**R√©sultat attendu :**
```
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process
INFO:     Started server process
INFO:     Waiting for application startup.
INFO:     Modele charge avec succes depuis model/churn_model.pkl
INFO:     Application startup complete.
```

### Test 3 : Tester l'endpoint racine

**Dans un nouveau terminal :**
```bash
# Test avec curl
curl http://localhost:8000/

# Ou avec PowerShell
Invoke-WebRequest -Uri http://localhost:8000/ -UseBasicParsing | Select-Object -ExpandProperty Content
```

**R√©sultat attendu :**
```json
{
  "message": "Bank Churn Prediction API",
  "version": "1.0.0",
  "status": "running",
  "docs": "/docs"
}
```

### Test 4 : Tester le health check

```bash
curl http://localhost:8000/health
```

**R√©sultat attendu :**
```json
{
  "status": "healthy",
  "model_loaded": true
}
```

### Test 5 : Tester la documentation interactive

1. Ouvrir http://localhost:8000/docs dans le navigateur
2. V√©rifier que tous les endpoints sont visibles :
   - `GET /`
   - `GET /health`
   - `POST /predict`
   - `POST /predict/batch`
3. Cliquer sur `/predict` ‚Üí "Try it out"
4. Utiliser les donn√©es d'exemple :
```json
{
  "CreditScore": 650,
  "Age": 35,
  "Tenure": 5,
  "Balance": 50000,
  "NumOfProducts": 2,
  "HasCrCard": 1,
  "IsActiveMember": 1,
  "EstimatedSalary": 75000,
  "Geography_Germany": 0,
  "Geography_Spain": 1
}
```
5. Cliquer sur "Execute"
6. V√©rifier la r√©ponse :
```json
{
  "churn_probability": 0.XXXX,
  "prediction": 0,
  "risk_level": "Low" ou "Medium" ou "High"
}
```

### Test 6 : Tester une pr√©diction avec curl

```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "CreditScore": 700,
    "Age": 40,
    "Tenure": 7,
    "Balance": 80000,
    "NumOfProducts": 3,
    "HasCrCard": 1,
    "IsActiveMember": 0,
    "EstimatedSalary": 90000,
    "Geography_Germany": 1,
    "Geography_Spain": 0
  }'
```

**R√©sultat attendu :** JSON avec churn_probability, prediction, risk_level

### Test 7 : Tester la validation (donn√©es invalides)

```bash
# Test avec donn√©es invalides (√¢ge trop √©lev√©)
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "CreditScore": 700,
    "Age": 150,
    "Tenure": 5,
    "Balance": 50000,
    "NumOfProducts": 2,
    "HasCrCard": 1,
    "IsActiveMember": 1,
    "EstimatedSalary": 70000,
    "Geography_Germany": 0,
    "Geography_Spain": 1
  }'
```

**R√©sultat attendu :** Erreur 422 avec message de validation

### Test 8 : Tester le batch prediction

```bash
curl -X POST "http://localhost:8000/predict/batch" \
  -H "Content-Type: application/json" \
  -d '[
    {
      "CreditScore": 750,
      "Age": 30,
      "Tenure": 3,
      "Balance": 25000,
      "NumOfProducts": 2,
      "HasCrCard": 1,
      "IsActiveMember": 1,
      "EstimatedSalary": 60000,
      "Geography_Germany": 0,
      "Geography_Spain": 0
    },
    {
      "CreditScore": 500,
      "Age": 55,
      "Tenure": 8,
      "Balance": 150000,
      "NumOfProducts": 4,
      "HasCrCard": 0,
      "IsActiveMember": 0,
      "EstimatedSalary": 120000,
      "Geography_Germany": 1,
      "Geography_Spain": 0
    }
  ]'
```

**R√©sultat attendu :** JSON avec un tableau de pr√©dictions

---

## ‚úÖ MODULE 3 : Docker

### Test 1 : V√©rifier Docker

```bash
# V√©rifier que Docker fonctionne
docker --version
docker ps
```

### Test 2 : V√©rifier le Dockerfile

```bash
# Lire le Dockerfile
cat Dockerfile
# ou sur Windows
type Dockerfile
```

**V√©rifier que :**
- Base image : `python:3.9-slim`
- Port expos√© : `8000`
- Commande : `uvicorn app.main:app --host 0.0.0.0 --port 8000`

### Test 3 : Build l'image Docker

```bash
# Build l'image
docker build -t churn-api:v1 .

# V√©rifier que l'image a √©t√© cr√©√©e
docker images | grep churn-api
```

**R√©sultat attendu :**
```
churn-api   v1   [IMAGE_ID]   [Taille]   [Date]
```

### Test 4 : Tester le conteneur localement

```bash
# Lancer le conteneur
docker run -d -p 8000:8000 --name churn-api-test churn-api:v1

# V√©rifier que le conteneur tourne
docker ps | grep churn-api-test

# Tester l'API dans le conteneur
curl http://localhost:8000/health
```

**R√©sultat attendu :**
```json
{
  "status": "healthy",
  "model_loaded": true
}
```

### Test 5 : V√©rifier les logs du conteneur

```bash
# Voir les logs
docker logs churn-api-test
```

**R√©sultat attendu :** Logs montrant le d√©marrage de l'API et le chargement du mod√®le

### Test 6 : Arr√™ter et nettoyer

```bash
# Arr√™ter le conteneur
docker stop churn-api-test

# Supprimer le conteneur
docker rm churn-api-test
```

---

## ‚úÖ MODULE 4 : D√©ploiement Azure

### Pr√©requis

```bash
# V√©rifier Azure CLI
az --version

# Se connecter √† Azure
az login

# V√©rifier l'abonnement
az account show
```

### Test 1 : V√©rifier les ressources Azure existantes

```bash
# Lister les resource groups
az group list --output table

# V√©rifier si votre resource group existe
az group show --name rg-mlops-bank-churn
```

### Test 2 : Tester le script de d√©ploiement (optionnel - attention aux co√ªts)

**‚ö†Ô∏è ATTENTION :** Ne lancez ce script que si vous voulez cr√©er de nouvelles ressources Azure (co√ªts possibles)

```powershell
# Sur Windows PowerShell
.\deploy-azure.ps1
```

**Ou cr√©er un script de test qui v√©rifie seulement :**

```bash
# Cr√©er un script test-azure.sh
cat > test-azure.sh << 'EOF'
#!/bin/bash
echo "Test des pr√©requis Azure..."

# V√©rifier la connexion
az account show || { echo "‚ùå Non connect√© √† Azure"; exit 1; }
echo "‚úÖ Connect√© √† Azure"

# V√©rifier les providers
az provider list --query "[?namespace=='Microsoft.ContainerRegistry' || namespace=='Microsoft.App'].{Namespace:namespace, State:registrationState}" --output table
echo "‚úÖ Providers v√©rifi√©s"

echo "‚úÖ Tous les pr√©requis sont OK"
EOF

chmod +x test-azure.sh
./test-azure.sh
```

### Test 3 : V√©rifier l'API en production (si d√©j√† d√©ploy√©e)

```bash
# R√©cup√©rer l'URL de l'application
# (Depuis azure-deploy-info.txt ou le portail Azure)
APP_URL="https://bank-churn.delightfulflower-76ee4057.francecentral.azurecontainerapps.io"

# Tester le health check
curl $APP_URL/health

# Tester une pr√©diction
curl -X POST "$APP_URL/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "CreditScore": 700,
    "Age": 35,
    "Tenure": 5,
    "Balance": 50000,
    "NumOfProducts": 2,
    "HasCrCard": 1,
    "IsActiveMember": 1,
    "EstimatedSalary": 70000,
    "Geography_Germany": 0,
    "Geography_Spain": 1
  }'
```

### Test 4 : V√©rifier les logs Azure

```bash
# Voir les logs de la Container App
az containerapp logs show \
  --name bank-churn \
  --resource-group rg-mlops-bank-churn \
  --tail 50
```

---

## ‚úÖ MODULE 5 : CI/CD avec GitHub Actions

### Test 1 : V√©rifier le workflow

```bash
# V√©rifier que le fichier workflow existe
cat .github/workflows/ci-cd.yml
```

**V√©rifier que le workflow contient :**
- Trigger sur `push` vers `main`
- Job `test` avec matrix Python
- Job `build-and-deploy`
- Job `notify`

### Test 2 : V√©rifier les secrets GitHub (via interface web)

**Actions √† faire :**
1. Aller sur GitHub ‚Üí Votre repo ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions
2. V√©rifier que ces secrets existent :
   - `AZURE_TENANT_ID`
   - `AZURE_CLIENT_ID`
   - `AZURE_CLIENT_SECRET`
   - `AZURE_RESOURCE_GROUP`
   - `AZURE_CONTAINER_APP_NAME`
   - `AZURE_REGISTRY_NAME`

### Test 3 : Tester les tests localement

```bash
# Installer les d√©pendances de test
pip install pytest pytest-cov flake8

# Ex√©cuter les tests
pytest tests/ -v

# Avec coverage
pytest tests/ --cov=app --cov-report=html

# Linting
flake8 app/ --count --select=E9,F63,F7,F82 --show-source --statistics
```

**R√©sultat attendu :** Tous les tests passent ‚úÖ

### Test 4 : D√©clencher le pipeline (test)

**Option 1 : Faire un petit changement et commit**

```bash
# Faire un petit changement
echo "# Test CI/CD" >> README.md

# Commit et push
git add README.md
git commit -m "Test: D√©clencher le pipeline CI/CD"
git push origin main
```

**Option 2 : Utiliser l'interface GitHub**

1. Aller sur GitHub ‚Üí Actions
2. S√©lectionner le workflow "CI/CD Pipeline"
3. Cliquer sur "Run workflow"
4. S√©lectionner la branche `main`
5. Cliquer sur "Run workflow"

### Test 5 : Observer le pipeline

1. Aller sur GitHub ‚Üí Actions
2. Cliquer sur le run en cours
3. Observer les jobs :
   - ‚úÖ `test` doit passer
   - ‚úÖ `build-and-deploy` doit passer
   - ‚úÖ `notify` doit passer

### Test 6 : V√©rifier les artifacts

Dans GitHub Actions, v√©rifier que :
- Les tests ont g√©n√©r√© un rapport de coverage
- Le build Docker a r√©ussi
- Le d√©ploiement Azure a r√©ussi

---

## üß™ TESTS AUTOMATIS√âS

### Ex√©cuter tous les tests

```bash
# Cr√©er un script de test complet
cat > test-all.sh << 'EOF'
#!/bin/bash

echo "üß™ Tests complets du projet MLOps"
echo "=================================="

# Test 1: Module 1 - Entra√Ænement
echo ""
echo "üìä Test Module 1: Entra√Ænement"
python train_model.py
if [ $? -eq 0 ]; then
    echo "‚úÖ Module 1: OK"
else
    echo "‚ùå Module 1: √âCHEC"
fi

# Test 2: Module 2 - API
echo ""
echo "üöÄ Test Module 2: API"
# D√©marrer l'API en arri√®re-plan
uvicorn app.main:app --host 0.0.0.0 --port 8000 &
API_PID=$!
sleep 5

# Tester les endpoints
curl -f http://localhost:8000/health > /dev/null 2>&1
if [ $? -eq 0 ]; then
    echo "‚úÖ Module 2: API fonctionne"
else
    echo "‚ùå Module 2: API ne r√©pond pas"
fi

# Arr√™ter l'API
kill $API_PID

# Test 3: Module 3 - Docker
echo ""
echo "üê≥ Test Module 3: Docker"
docker build -t churn-api:test . > /dev/null 2>&1
if [ $? -eq 0 ]; then
    echo "‚úÖ Module 3: Docker build OK"
else
    echo "‚ùå Module 3: Docker build √âCHEC"
fi

# Test 4: Module 5 - Tests unitaires
echo ""
echo "üî¨ Test Module 5: Tests unitaires"
pytest tests/ -v --tb=short
if [ $? -eq 0 ]; then
    echo "‚úÖ Module 5: Tous les tests passent"
else
    echo "‚ùå Module 5: Certains tests √©chouent"
fi

echo ""
echo "=================================="
echo "‚úÖ Tests termin√©s"
EOF

chmod +x test-all.sh
./test-all.sh
```

**Version PowerShell pour Windows :**

```powershell
# Cr√©er test-all.ps1
@"
# Tests complets du projet MLOps
Write-Host "üß™ Tests complets du projet MLOps" -ForegroundColor Cyan
Write-Host "==================================" -ForegroundColor Cyan

# Test 1: Module 1
Write-Host "`nüìä Test Module 1: Entra√Ænement" -ForegroundColor Yellow
python train_model.py
if ($LASTEXITCODE -eq 0) {
    Write-Host "‚úÖ Module 1: OK" -ForegroundColor Green
} else {
    Write-Host "‚ùå Module 1: √âCHEC" -ForegroundColor Red
}

# Test 2: Module 2 - API
Write-Host "`nüöÄ Test Module 2: API" -ForegroundColor Yellow
Start-Process -NoNewWindow -FilePath "uvicorn" -ArgumentList "app.main:app --host 0.0.0.0 --port 8000"
Start-Sleep -Seconds 5

$response = Invoke-WebRequest -Uri "http://localhost:8000/health" -UseBasicParsing -ErrorAction SilentlyContinue
if ($response.StatusCode -eq 200) {
    Write-Host "‚úÖ Module 2: API fonctionne" -ForegroundColor Green
} else {
    Write-Host "‚ùå Module 2: API ne r√©pond pas" -ForegroundColor Red
}

# Test 3: Module 3 - Docker
Write-Host "`nüê≥ Test Module 3: Docker" -ForegroundColor Yellow
docker build -t churn-api:test . 2>&1 | Out-Null
if ($LASTEXITCODE -eq 0) {
    Write-Host "‚úÖ Module 3: Docker build OK" -ForegroundColor Green
} else {
    Write-Host "‚ùå Module 3: Docker build √âCHEC" -ForegroundColor Red
}

# Test 4: Module 5 - Tests unitaires
Write-Host "`nüî¨ Test Module 5: Tests unitaires" -ForegroundColor Yellow
pytest tests/ -v
if ($LASTEXITCODE -eq 0) {
    Write-Host "‚úÖ Module 5: Tous les tests passent" -ForegroundColor Green
} else {
    Write-Host "‚ùå Module 5: Certains tests √©chouent" -ForegroundColor Red
}

Write-Host "`n==================================" -ForegroundColor Cyan
Write-Host "‚úÖ Tests termin√©s" -ForegroundColor Green
"@ | Out-File -FilePath test-all.ps1 -Encoding UTF8

# Ex√©cuter
.\test-all.ps1
```

---

## üìã CHECKLIST DE TEST COMPL√àTE

### Module 1 : Entra√Ænement
- [ ] Dataset pr√©sent et valide
- [ ] Script d'entra√Ænement s'ex√©cute sans erreur
- [ ] Mod√®le sauvegard√© dans `model/churn_model.pkl`
- [ ] MLflow UI accessible et montre les runs
- [ ] M√©triques affich√©es (accuracy, precision, recall, etc.)

### Module 2 : API FastAPI
- [ ] API d√©marre sans erreur
- [ ] Endpoint `/` r√©pond
- [ ] Endpoint `/health` r√©pond avec `model_loaded: true`
- [ ] Endpoint `/predict` fonctionne avec donn√©es valides
- [ ] Endpoint `/predict` rejette les donn√©es invalides (422)
- [ ] Endpoint `/predict/batch` fonctionne
- [ ] Documentation `/docs` accessible et fonctionnelle

### Module 3 : Docker
- [ ] Dockerfile pr√©sent et valide
- [ ] Build Docker r√©ussit
- [ ] Image Docker cr√©√©e
- [ ] Conteneur d√©marre et API fonctionne dedans
- [ ] Port 8000 expos√© correctement

### Module 4 : Azure
- [ ] Azure CLI install√© et connect√©
- [ ] Resource group existe (si d√©j√† d√©ploy√©)
- [ ] API en production accessible (si d√©j√† d√©ploy√©e)
- [ ] Health check fonctionne en production
- [ ] Pr√©dictions fonctionnent en production

### Module 5 : CI/CD
- [ ] Fichier workflow pr√©sent dans `.github/workflows/`
- [ ] Secrets GitHub configur√©s
- [ ] Tests unitaires passent localement
- [ ] Pipeline se d√©clenche sur commit
- [ ] Tous les jobs du pipeline passent
- [ ] D√©ploiement automatique fonctionne

---

## üö® R√âSOLUTION DE PROBL√àMES

### Probl√®me : Le mod√®le ne charge pas

```bash
# V√©rifier que le mod√®le existe
ls -lh model/churn_model.pkl

# Si absent, r√©entra√Æner
python train_model.py
```

### Probl√®me : L'API ne d√©marre pas

```bash
# V√©rifier que le port 8000 n'est pas utilis√©
# Windows :
netstat -ano | findstr :8000
# Mac/Linux :
lsof -i :8000

# V√©rifier les d√©pendances
pip install -r requirements.txt
```

### Probl√®me : Docker build √©choue

```bash
# V√©rifier le Dockerfile
cat Dockerfile

# Build avec plus de logs
docker build -t churn-api:v1 . --progress=plain

# V√©rifier que le mod√®le est copi√©
docker run --rm churn-api:v1 ls -la model/
```

### Probl√®me : Tests √©chouent

```bash
# Ex√©cuter avec plus de d√©tails
pytest tests/ -v -s

# Ex√©cuter un test sp√©cifique
pytest tests/test_api.py::TestHealthEndpoints::test_root_endpoint -v
```

### Probl√®me : Pipeline CI/CD √©choue

1. V√©rifier les logs dans GitHub Actions
2. V√©rifier que les secrets sont configur√©s
3. Tester localement les commandes qui √©chouent
4. V√©rifier les noms de ressources Azure

---

## ‚úÖ VALIDATION FINALE AVANT SOUTENANCE

```bash
# Script de validation compl√®te
echo "üîç Validation finale du projet..."
echo ""

echo "1. V√©rification des fichiers essentiels..."
[ -f "train_model.py" ] && echo "‚úÖ train_model.py" || echo "‚ùå train_model.py manquant"
[ -f "app/main.py" ] && echo "‚úÖ app/main.py" || echo "‚ùå app/main.py manquant"
[ -f "Dockerfile" ] && echo "‚úÖ Dockerfile" || echo "‚ùå Dockerfile manquant"
[ -f ".github/workflows/ci-cd.yml" ] && echo "‚úÖ ci-cd.yml" || echo "‚ùå ci-cd.yml manquant"
[ -f "model/churn_model.pkl" ] && echo "‚úÖ Mod√®le sauvegard√©" || echo "‚ùå Mod√®le manquant - Ex√©cutez train_model.py"

echo ""
echo "2. Tests unitaires..."
pytest tests/ -v --tb=short

echo ""
echo "3. Build Docker..."
docker build -t churn-api:validation . > /dev/null 2>&1
if [ $? -eq 0 ]; then
    echo "‚úÖ Docker build OK"
else
    echo "‚ùå Docker build √©choue"
fi

echo ""
echo "‚úÖ Validation termin√©e !"
```

---

**Bonne chance pour vos tests ! üöÄ**
